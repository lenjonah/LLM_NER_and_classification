{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask\n",
    "\n",
    "Finetune a pretrained BERT model to do both named entity recognition and sequence level classification in a single forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeUQqc64Oyk6",
    "outputId": "5b26637a-28fc-471c-9b4d-4bea7444e991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# if running in a google colab, make sure the relevant packages are trained\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GydJCjoxO53-"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_MPdQWUO7hm"
   },
   "outputs": [],
   "source": [
    "# function to align labels with tokens, taken from NER.ipynb\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUQ4zVfqO_O8",
    "outputId": "dceb9a03-ae9d-4d91-d396-47b84f219078"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# function to evaluate perfomance metrics (precision, recall, f1-score, accuracy) for given predictions and ground-truth labels\n",
    "def compute_metrics(predictions, labels):\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw9lOLwEPBVe"
   },
   "outputs": [],
   "source": [
    "# specify multitask model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_seq_labels=5, num_token_labels=9):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        # size of the hidden layer of the BERT model\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        # here: learnable parameters to combine the losses of both tasks\n",
    "        self.log_sigma1 = nn.Parameter(torch.tensor(0.0))  # For sequence loss\n",
    "        self.log_sigma2 = nn.Parameter(torch.tensor(-1.4))  # For token loss\n",
    "\n",
    "        # Sequence classification head\n",
    "        self.seq_classifier = nn.Sequential(\n",
    "            #nn.Linear(hidden_size, hidden_size // 2),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_seq_labels)\n",
    "        )\n",
    "\n",
    "        # Token classification head\n",
    "        self.token_classifier = nn.Sequential(\n",
    "            #nn.Linear(hidden_size, hidden_size // 2),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_token_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # output of BERT model\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        # [CLS] token used for classification of the while sequence\n",
    "        cls_output = outputs.pooler_output           # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # sequence classification (using [CLS] token)\n",
    "        seq_logits = self.seq_classifier(cls_output)  # Shape: (batch_size, num_seq_labels)\n",
    "\n",
    "        # token classification (per token)\n",
    "        token_logits = self.token_classifier(sequence_output)  # Shape: (batch_size, seq_length, num_token_labels)\n",
    "\n",
    "        return seq_logits, token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXEaBEvQPDLF"
   },
   "outputs": [],
   "source": [
    "# create custom huggingface dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, seq_labels, token_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.seq_labels = seq_labels\n",
    "        self.token_labels = token_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'seq_label': torch.tensor(self.seq_labels[idx], dtype=torch.long),\n",
    "            'token_labels': torch.tensor(self.token_labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# data collator\n",
    "# make sure all vectors are correctly padded and have the right dimension\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    seq_labels = torch.stack([item['seq_label'] for item in batch])\n",
    "    token_labels = pad_sequence([item['token_labels'] for item in batch], batch_first=True, padding_value=-100)  # Ignore padding tokens\n",
    "    return input_ids, attention_mask, seq_labels, token_labels\n",
    "\n",
    "# training Loop\n",
    "def train_model(model, training_dataloader, validation_dataloader, optimizer, lr_scheduler, accelerator, device, epochs=3, alpha=0.5, beta=0.5):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # initialize losses\n",
    "        total_seq_loss = 0\n",
    "        total_token_loss = 0\n",
    "\n",
    "        # loop over batches from dataloader\n",
    "        loop = tqdm(training_dataloader, leave=True)\n",
    "        for batch in loop:\n",
    "            # get samples and labels for both tokens and sequences\n",
    "            input_ids, attention_mask, seq_labels, token_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            # set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            seq_logits, token_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # loss computation\n",
    "            seq_loss = F.cross_entropy(seq_logits, seq_labels)\n",
    "            token_loss = F.cross_entropy(token_logits.view(-1, token_logits.size(-1)),\n",
    "                                         F.pad(token_labels, (0, token_logits.size(1) - token_labels.size(1)), \"constant\", -100).view(-1), ignore_index=-100)\n",
    "\n",
    "            # combine both losses using the trainable parameters\n",
    "            weighted_seq_loss = 1/(2 * torch.exp(model.log_sigma1)) * seq_loss + model.log_sigma1\n",
    "            weighted_token_loss = 1/(torch.exp(model.log_sigma2)) * token_loss + model.log_sigma2\n",
    "            loss = weighted_seq_loss + weighted_token_loss\n",
    "\n",
    "            # backward pass\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # update metrics\n",
    "            total_seq_loss += seq_loss.item()\n",
    "            total_token_loss += token_loss.item()\n",
    "\n",
    "            # update progress bar\n",
    "            loop.set_description(f'Epoch {epoch + 1}')\n",
    "            loop.set_postfix(seq_loss=seq_loss.item(), token_loss=token_loss.item())\n",
    "\n",
    "        # compute validation loss\n",
    "        with torch.no_grad():\n",
    "          \n",
    "          # initialize sequnce loss and token loss to zero\n",
    "          total_seq_loss_val = 0\n",
    "          total_token_loss_val = 0\n",
    "          loop = tqdm(validation_dataloader, leave=True)\n",
    "\n",
    "          all_seq_preds = []\n",
    "          all_seq_labels = []\n",
    "\n",
    "          all_token_preds = []\n",
    "          all_token_labels = []\n",
    "\n",
    "          # loop over batches\n",
    "          for batch in loop:\n",
    "              # get samples and labels for both tokens and sequences\n",
    "              input_ids, attention_mask, seq_labels, token_labels = [x.to(device) for x in batch]\n",
    "\n",
    "              # forward pass\n",
    "              seq_logits, token_logits = model(input_ids, attention_mask)\n",
    "\n",
    "              all_seq_preds = np.append(all_seq_preds, seq_logits.argmax(-1).cpu())\n",
    "              all_seq_labels = np.append(all_seq_labels, seq_labels.cpu())\n",
    "\n",
    "              all_token_preds.append(token_logits.argmax(-1).cpu())\n",
    "              all_token_labels.append(F.pad(token_labels.cpu(), (0, token_logits.size(1) - token_labels.size(1)), \"constant\", -100))\n",
    "\n",
    "              # loss computation\n",
    "              seq_loss = F.cross_entropy(seq_logits, seq_labels)\n",
    "              token_loss = F.cross_entropy(token_logits.view(-1, token_logits.size(-1)),\n",
    "                                            F.pad(token_labels, (0, token_logits.size(1) - token_labels.size(1)), \"constant\", -100).view(-1), ignore_index=-100)\n",
    "\n",
    "              # update validation metrics\n",
    "              total_seq_loss_val += seq_loss.item()\n",
    "              total_token_loss_val += token_loss.item()\n",
    "\n",
    "          # print training and validation losses\n",
    "          print(f\"Epoch {epoch+1}/{epochs}, Training Sequence Loss: {total_seq_loss/len(training_dataloader)}, Training Token Loss: {total_token_loss/len(training_dataloader)}\")\n",
    "          print(f\"\\t Validation Sequence Loss: {total_seq_loss_val/len(validation_dataloader)}, Validation Token Loss: {total_token_loss_val/len(validation_dataloader)}\")\n",
    "\n",
    "          # compute and print validation performance metrics\n",
    "          f1_seq = f1_score(all_seq_labels, all_seq_preds, average=\"weighted\")\n",
    "          acc_seq = accuracy_score(all_seq_labels, all_seq_preds)\n",
    "          print(f\"\\t Validation Sequence F_1: {f1_seq}, Accuracy: {acc_seq}\")\n",
    "          all_token_preds = np.vstack(all_token_preds)\n",
    "          all_token_labels = np.vstack(all_token_labels)\n",
    "          metrics_token = compute_metrics(all_token_preds, all_token_labels)\n",
    "          print(f\"\\t Validation Token F_1: {metrics_token['f1']}, Accuracy: {metrics_token['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9UEbzYnPGWT",
    "outputId": "2371a036-0b6e-4923-c979-ef804a331ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# function to form a sequence from a list of strings\n",
    "# from Classification.ipynb\n",
    "def smart_join(strings):\n",
    "    result = []\n",
    "    for i, s in enumerate(strings):\n",
    "        # Add a space before appending if the previous item is not a special character\n",
    "        if i > 0 and not strings[i-1].endswith(('(', ' ')) and not s.startswith(('.', ',',';', ':', '%', \"\"\"'\"\"\", '''\"''', ')', '!', '?')):\n",
    "            result.append(' ')\n",
    "\n",
    "        result.append(s)\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "# load datasets\n",
    "ds_train = load_dataset(\"conll2003\", split=\"train\")\n",
    "ds_valid = load_dataset(\"conll2003\", split=\"validation\")\n",
    "ds_test = load_dataset(\"conll2003\", split=\"test\")\n",
    "\n",
    "# load NER labels\n",
    "ner_feature = ds_train.features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "\n",
    "# training dataset\n",
    "training_classification = ds_train.shuffle(seed=23).select(range(1000))\n",
    "training_tokens = training_classification['tokens']\n",
    "\n",
    "training_sentences = []\n",
    "for t in training_tokens:\n",
    "  sentence = smart_join(t)\n",
    "  training_sentences.append(sentence)\n",
    "\n",
    "# sequence level labels generated in Classification.ipynb\n",
    "training_sentence_labels = [2, 4, 4, 1, 4, 4, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 0, 2, 4, 4, 1, 4, 4, 4, 2, 1, 1, 0, 4, 1, 2, 1, 0, 4, 4, 1, 4, 1, 0, 0, 1, 1, 0, 1, 0, 4, 1, 1, 1, 2, 1, 4, 4, 0, 1, 0, 2, 4, 4, 4, 1, 2, 0, 1, 1, 1, 0, 4, 1, 1, 2, 4, 4, 0, 1, 2, 1, 4, 1, 4, 1, 2, 0, 0, 1, 1, 4, 1, 2, 4, 0, 1, 1, 4, 4, 0, 2, 1, 4, 4, 0, 1, 0, 2, 4, 1, 1, 1, 2, 1, 1, 4, 0, 1, 1, 0, 4, 4, 1, 0, 2, 1, 2, 1, 0, 0, 1, 1, 1, 2, 4, 1, 1, 2, 2, 1, 0, 1, 1, 4, 2, 2, 1, 4, 2, 3, 4, 4, 0, 2, 4, 1, 0, 4, 1, 4, 4, 0, 4, 4, 1, 2, 1, 1, 1, 0, 4, 0, 1, 2, 2, 1, 4, 2, 2, 2, 2, 1, 4, 1, 2, 4, 0, 2, 4, 1, 2, 2, 4, 2, 4, 4, 0, 2, 0, 4, 4, 4, 4, 2, 1, 4, 1, 1, 2, 1, 4, 0, 1, 4, 2, 2, 4, 0, 2, 0, 1, 1, 1, 4, 4, 0, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 0, 1, 4, 4, 4, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1, 4, 4, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 4, 4, 2, 4, 2, 4, 0, 4, 1, 0, 1, 4, 2, 4, 4, 4, 4, 4, 0, 1, 1, 4, 1, 4, 0, 1, 1, 2, 1, 0, 0, 0, 4, 4, 1, 4, 2, 4, 1, 0, 4, 4, 0, 0, 0, 2, 4, 1, 2, 2, 1, 3, 4, 4, 2, 0, 0, 0, 4, 1, 0, 1, 4, 2, 2, 1, 4, 0, 0, 4, 0, 1, 4, 1, 0, 1, 4, 4, 4, 4, 0, 1, 0, 4, 1, 1, 0, 4, 0, 4, 1, 4, 2, 4, 2, 1, 4, 1, 4, 0, 1, 4, 0, 4, 0, 1, 0, 1, 4, 1, 2, 2, 4, 2, 2, 0, 4, 1, 4, 1, 2, 1, 4, 1, 2, 2, 1, 4, 4, 4, 2, 1, 4, 4, 0, 4, 4, 1, 1, 2, 2, 2, 1, 4, 1, 4, 1, 4, 0, 2, 1, 2, 4, 4, 0, 1, 2, 0, 0, 2, 0, 4, 2, 4, 1, 0, 2, 0, 2, 4, 2, 0, 0, 4, 0, 4, 1, 4, 1, 0, 1, 4, 0, 4, 4, 1, 0, 0, 0, 2, 1, 1, 4, 0, 2, 0, 1, 1, 4, 4, 0, 1, 1, 4, 1, 0, 1, 1, 1, 1, 0, 4, 1, 4, 4, 1, 2, 4, 2, 1, 1, 1, 1, 4, 4, 1, 2, 0, 1, 1, 1, 1, 1, 1, 4, 1, 2, 4, 0, 4, 1, 4, 1, 1, 0, 0, 2, 1, 4, 1, 4, 2, 0, 4, 0, 0, 1, 1, 4, 4, 2, 1, 1, 1, 0, 4, 2, 2, 0, 1, 4, 4, 1, 4, 4, 2, 4, 4, 2, 1, 1, 0, 2, 4, 0, 2, 1, 4, 0, 4, 0, 1, 0, 4, 4, 4, 0, 4, 1, 1, 4, 2, 1, 4, 1, 4, 2, 1, 4, 1, 4, 1, 4, 4, 2, 4, 4, 0, 1, 0, 2, 2, 4, 4, 4, 0, 1, 2, 1, 4, 1, 1, 2, 1, 4, 2, 4, 4, 1, 1, 1, 0, 3, 4, 0, 4, 1, 4, 0, 4, 1, 2, 1, 1, 4, 0, 0, 4, 2, 4, 1, 2, 4, 0, 0, 4, 4, 1, 4, 4, 0, 2, 1, 1, 1, 4, 0, 0, 4, 1, 4, 0, 4, 4, 1, 4, 1, 2, 1, 2, 4, 0, 1, 4, 4, 4, 4, 4, 4, 4, 1, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 2, 1, 2, 0, 2, 4, 0, 4, 4, 1, 1, 4, 4, 2, 2, 1, 1, 4, 1, 2, 4, 1, 1, 0, 1, 0, 2, 0, 4, 4, 0, 2, 0, 1, 0, 1, 1, 2, 1, 2, 4, 4, 1, 1, 1, 1, 2, 4, 4, 4, 2, 2, 4, 0, 4, 4, 4, 4, 4, 0, 4, 1, 0, 4, 1, 4, 4, 0, 0, 1, 0, 1, 0, 1, 4, 1, 4, 1, 1, 2, 1, 0, 1, 4, 2, 0, 4, 0, 2, 4, 0, 4, 1, 1, 4, 1, 0, 0, 1, 1, 4, 4, 0, 1, 0, 1, 2, 1, 2, 0, 0, 2, 0, 4, 2, 4, 2, 0, 2, 1, 4, 2, 2, 0, 4, 1, 0, 4, 1, 1, 1, 1, 4, 4, 2, 4, 2, 2, 1, 4, 1, 2, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4, 2, 0, 4, 4, 1, 2, 1, 2, 0, 4, 4, 1, 0, 4, 4, 1, 1, 1, 1, 2, 4, 0, 1, 4, 1, 1, 0, 2, 0, 4, 1, 1, 0, 4, 4, 1, 1, 1, 4, 4, 1, 1, 1, 4, 1, 0, 1, 4, 0, 1, 1, 4, 4, 2, 2, 2, 0, 0, 2, 1, 4, 4, 1, 1, 2, 1, 1, 1, 0, 0, 4, 4, 0, 1, 0, 1, 4, 0, 1, 0, 4, 2, 0, 4, 1, 2, 2, 0, 1, 0, 1, 4, 2, 4, 0, 4, 2, 1, 4, 1, 0, 4, 4, 1, 1, 0, 4, 1, 0, 2, 4, 0, 1, 4, 0, 0, 4, 1, 1, 1, 2, 0, 4, 4, 0, 0, 4, 1, 4, 4, 1, 4, 0, 4, 4, 1, 1, 4, 1, 0, 4, 2, 0, 4, 1, 2, 0, 0, 4, 1, 4, 0, 0, 4, 1, 1, 1, 1, 4, 4, 1, 0, 4, 4, 4, 2, 4]\n",
    "\n",
    "training_token_labels = training_classification['ner_tags']\n",
    "\n",
    "\n",
    "# validation dataset\n",
    "validation_classification = ds_valid.shuffle(seed=23).select(range(200))\n",
    "validation_tokens = validation_classification['tokens']\n",
    "\n",
    "validation_sentences = []\n",
    "for t in validation_tokens:\n",
    "  sentence = smart_join(t)\n",
    "  validation_sentences.append(sentence)\n",
    "\n",
    "# sequence level labels generated in Classification.ipynb\n",
    "validation_sentence_labels = [0, 0, 1, 1, 4, 4, 1, 2, 1, 4, 2, 1, 0, 1, 0, 2, 1, 1, 1, 3, 1, 0, 4, 0, 4, 4, 3, 0, 1, 2, 1, 2, 0, 0, 1, 2, 1, 4, 1, 1, 4, 1, 1, 1, 1, 0, 4, 4, 2, 4, 2, 2, 1, 2, 1, 1, 2, 4, 1, 1, 4, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 4, 0, 4, 1, 0, 1, 0, 2, 4, 1, 0, 1, 1, 1, 1, 0, 1, 1, 4, 2, 1, 0, 4, 4, 4, 1, 4, 1, 0, 4, 1, 1, 0, 1, 0, 2, 1, 1, 4, 0, 1, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 1, 1, 4, 4, 4, 4, 0, 0, 2, 1, 4, 1, 1, 4, 0, 1, 4, 4, 1, 0, 1, 1, 0, 0, 4, 2, 4, 4, 4, 1, 4, 1, 4, 1, 0, 2, 4, 4, 2, 1, 4, 2, 4, 2, 1, 0, 0, 4, 1, 1, 0, 1, 4, 1, 2, 1, 2, 0, 1, 1, 4, 4, 4, 1, 4, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "validation_token_labels = validation_classification['ner_tags']\n",
    "\n",
    "\n",
    "# test dataset\n",
    "test_classification = ds_test.shuffle(seed=23).select(range(200))\n",
    "test_tokens = test_classification['tokens']\n",
    "\n",
    "test_sentences = []\n",
    "for t in test_tokens:\n",
    "  sentence = smart_join(t)\n",
    "  test_sentences.append(sentence)\n",
    "\n",
    "# sequence level labels generated in Classification.ipynb\n",
    "test_sentence_labels = [1, 1, 2, 2, 1, 4, 1, 4, 1, 1, 1, 2, 1, 4, 4, 2, 1, 0, 1, 4, 1, 4, 4, 1, 2, 1, 1, 0, 1, 1, 1, 2, 4, 1, 4, 0, 1, 0, 2, 4, 1, 2, 4, 2, 0, 1, 1, 2, 4, 1, 4, 2, 2, 4, 4, 2, 4, 1, 4, 1, 2, 0, 1, 4, 1, 2, 0, 4, 0, 2, 0, 2, 2, 1, 4, 4, 2, 4, 2, 1, 1, 1, 4, 2, 1, 1, 1, 1, 4, 4, 4, 1, 2, 1, 2, 1, 1, 1, 4, 2, 4, 1, 0, 1, 4, 2, 1, 1, 4, 0, 1, 2, 2, 4, 4, 1, 4, 4, 0, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 0, 1, 4, 4, 2, 1, 0, 4, 4, 4, 2, 1, 4, 0, 1, 1, 1, 1, 1, 1, 2, 4, 4, 1, 1, 4, 4, 2, 0, 4, 2, 1, 4, 4, 1, 0, 0, 4, 0, 4, 2, 2, 4, 2, 1, 1, 1, 4, 1, 0, 1, 1, 1, 1, 0, 2, 4, 1, 4, 4, 4, 4, 1, 4, 3, 4, 2, 1]\n",
    "\n",
    "test_token_labels = test_classification['ner_tags']\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "max_length = max([len(l) for l in np.concatenate([training_sentences, validation_sentences, test_sentences])])\n",
    "\n",
    "# load BERT tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# align labels with tokens\n",
    "# use original word ids, before joining tokens into sentences\n",
    "training_token_labels = [align_labels_with_tokens(t, tokenizer(training_classification[i]['tokens'], is_split_into_words=True).word_ids(0) ) for i, t in enumerate(training_token_labels)]\n",
    "validation_token_labels = [align_labels_with_tokens(t, tokenizer(validation_classification[i]['tokens'], is_split_into_words=True).word_ids(0) ) for i, t in enumerate(validation_token_labels)]\n",
    "test_token_labels = [align_labels_with_tokens(t, tokenizer(test_classification[i]['tokens'], is_split_into_words=True).word_ids(0) ) for i, t in enumerate(test_token_labels)]\n",
    "\n",
    "# create datasets and dataloaders\n",
    "training_dataset = CustomDataset(training_sentences, training_sentence_labels, training_token_labels, tokenizer, max_length)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset = CustomDataset(validation_sentences, validation_sentence_labels, validation_token_labels, tokenizer, max_length)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = CustomDataset(test_sentences, test_sentence_labels, test_token_labels, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKuqc3ghPKTO",
    "outputId": "1b9a5d59-acd5-4de1-96dc-51e48f8e007d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16/16 [00:44<00:00,  2.77s/it, seq_loss=1.12, token_loss=0.733]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14, Training Sequence Loss: 1.420078121125698, Training Token Loss: 1.07689917832613\n",
      "\t Validation Sequence Loss: 1.201205462217331, Validation Token Loss: 0.6254969537258148\n",
      "\t Validation Sequence F_1: 0.44054083343557027, Accuracy: 0.515\n",
      "\t Validation Token F_1: 0.008810572687224669, Accuracy: 0.7961732660111613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 16/16 [00:43<00:00,  2.71s/it, seq_loss=0.815, token_loss=0.339]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/14, Training Sequence Loss: 1.076890666037798, Training Token Loss: 0.4318919759243727\n",
      "\t Validation Sequence Loss: 0.8945852220058441, Validation Token Loss: 0.34693189710378647\n",
      "\t Validation Sequence F_1: 0.6430362071649988, Accuracy: 0.695\n",
      "\t Validation Token F_1: 0.5472636815920399, Accuracy: 0.9019399415360085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.6, token_loss=0.159]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/14, Training Sequence Loss: 0.7959059812128544, Training Token Loss: 0.22457178216427565\n",
      "\t Validation Sequence Loss: 0.7253954857587814, Validation Token Loss: 0.21813618391752243\n",
      "\t Validation Sequence F_1: 0.7058842058110888, Accuracy: 0.735\n",
      "\t Validation Token F_1: 0.6952141057934509, Accuracy: 0.9431304809992027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.561, token_loss=0.1]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/14, Training Sequence Loss: 0.6369500085711479, Training Token Loss: 0.12317322427406907\n",
      "\t Validation Sequence Loss: 0.5312941074371338, Validation Token Loss: 0.20217426493763924\n",
      "\t Validation Sequence F_1: 0.7973087443432294, Accuracy: 0.805\n",
      "\t Validation Token F_1: 0.7586206896551725, Accuracy: 0.9492426255647091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.25, token_loss=0.0572]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/14, Training Sequence Loss: 0.44866914488375187, Training Token Loss: 0.07945937383919954\n",
      "\t Validation Sequence Loss: 0.5153326615691185, Validation Token Loss: 0.1460735034197569\n",
      "\t Validation Sequence F_1: 0.7925536020714619, Accuracy: 0.795\n",
      "\t Validation Token F_1: 0.8025806451612904, Accuracy: 0.9646558596864204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 16/16 [00:44<00:00,  2.75s/it, seq_loss=0.183, token_loss=0.0457]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/14, Training Sequence Loss: 0.377736022695899, Training Token Loss: 0.05204859480727464\n",
      "\t Validation Sequence Loss: 0.38438572362065315, Validation Token Loss: 0.16176610626280308\n",
      "\t Validation Sequence F_1: 0.8718205404112865, Accuracy: 0.875\n",
      "\t Validation Token F_1: 0.8167539267015708, Accuracy: 0.9670475684294446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 16/16 [00:43<00:00,  2.74s/it, seq_loss=0.152, token_loss=0.0324]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/14, Training Sequence Loss: 0.2841995498165488, Training Token Loss: 0.036941458587534726\n",
      "\t Validation Sequence Loss: 0.3669581972062588, Validation Token Loss: 0.13343499787151814\n",
      "\t Validation Sequence F_1: 0.8575756703160995, Accuracy: 0.86\n",
      "\t Validation Token F_1: 0.8270481144343304, Accuracy: 0.966781823013553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.0711, token_loss=0.0273]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/14, Training Sequence Loss: 0.21502772439271212, Training Token Loss: 0.030915238487068564\n",
      "\t Validation Sequence Loss: 0.3694503456354141, Validation Token Loss: 0.12621749378740788\n",
      "\t Validation Sequence F_1: 0.863428058765532, Accuracy: 0.865\n",
      "\t Validation Token F_1: 0.8441558441558441, Accuracy: 0.9720967313313845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.0637, token_loss=0.0231]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/14, Training Sequence Loss: 0.18199307518079877, Training Token Loss: 0.02274439320899546\n",
      "\t Validation Sequence Loss: 0.3297599144279957, Validation Token Loss: 0.13528584502637386\n",
      "\t Validation Sequence F_1: 0.8777772260840782, Accuracy: 0.88\n",
      "\t Validation Token F_1: 0.8426527958387515, Accuracy: 0.9691735317565772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.0632, token_loss=0.0213]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/14, Training Sequence Loss: 0.14171697618439794, Training Token Loss: 0.019768410187680274\n",
      "\t Validation Sequence Loss: 0.37890270724892616, Validation Token Loss: 0.11835752241313457\n",
      "\t Validation Sequence F_1: 0.8357888395351166, Accuracy: 0.84\n",
      "\t Validation Token F_1: 0.8470588235294119, Accuracy: 0.9712994950837098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 16/16 [00:43<00:00,  2.72s/it, seq_loss=0.0395, token_loss=0.02]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/14, Training Sequence Loss: 0.12068530544638634, Training Token Loss: 0.016409783391281962\n",
      "\t Validation Sequence Loss: 0.39519762992858887, Validation Token Loss: 0.12327207252383232\n",
      "\t Validation Sequence F_1: 0.8832088960312316, Accuracy: 0.885\n",
      "\t Validation Token F_1: 0.8597640891218873, Accuracy: 0.9731597129949509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 16/16 [00:43<00:00,  2.72s/it, seq_loss=0.0438, token_loss=0.0177]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/14, Training Sequence Loss: 0.10599148599430919, Training Token Loss: 0.01431423905887641\n",
      "\t Validation Sequence Loss: 0.3199357446283102, Validation Token Loss: 0.11794183775782585\n",
      "\t Validation Sequence F_1: 0.8811153552330023, Accuracy: 0.885\n",
      "\t Validation Token F_1: 0.8341968911917099, Accuracy: 0.9702365134201435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 16/16 [00:43<00:00,  2.73s/it, seq_loss=0.0585, token_loss=0.0173]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/14, Training Sequence Loss: 0.09256090852431953, Training Token Loss: 0.01303378288866952\n",
      "\t Validation Sequence Loss: 0.31463278643786907, Validation Token Loss: 0.1389664225280285\n",
      "\t Validation Sequence F_1: 0.896296712696389, Accuracy: 0.9\n",
      "\t Validation Token F_1: 0.8229166666666667, Accuracy: 0.968110550093011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 16/16 [00:43<00:00,  2.72s/it, seq_loss=0.0326, token_loss=0.0165]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/14, Training Sequence Loss: 0.08656661119312048, Training Token Loss: 0.013622149359434843\n",
      "\t Validation Sequence Loss: 0.27501474507153034, Validation Token Loss: 0.15115107223391533\n",
      "\t Validation Sequence F_1: 0.9062809164250882, Accuracy: 0.91\n",
      "\t Validation Token F_1: 0.8322496749024707, Accuracy: 0.9683762955089025\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01\n",
    "epochs = 14\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MultiTaskModel()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "num_update_steps_per_epoch = len(training_dataloader)\n",
    "num_training_steps = epochs * num_update_steps_per_epoch\n",
    "\n",
    "# linear weight decay\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, lr_scheduler = accelerator.prepare(\n",
    "     model, optimizer, training_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# start training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model, training_dataloader, validation_dataloader, optimizer, lr_scheduler, accelerator, device, epochs, alpha=1, beta=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ApcpgWePd9L",
    "outputId": "7aa88285-c4b4-4c6b-edfe-071a3f88696e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test \t Sequence Loss: 0.5435006022453308, Token Loss: 0.17799033969640732\n",
      "\t Sequence F_1: 0.8779372528317265, Accuracy: 0.88\n",
      "\t Token F_1: 0.834575260804769, Accuracy: 0.9630832841110455\n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# function to evaluate perfomance metrics (precision, recall, f1-score, accuracy) for given predictions and ground-truth labels\n",
    "def compute_metrics(predictions, labels):\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# compute performance on test set\n",
    "with torch.no_grad():\n",
    "\n",
    "  total_seq_loss_test = 0\n",
    "  total_token_loss_test = 0\n",
    "  loop = tqdm(test_dataloader, leave=True)\n",
    "\n",
    "  all_seq_preds = []\n",
    "  all_seq_labels = []\n",
    "\n",
    "  all_token_preds = []\n",
    "  all_token_labels = []\n",
    "\n",
    "  for batch in loop:\n",
    "      input_ids, attention_mask, seq_labels, token_labels = [x.to(device) for x in batch]\n",
    "\n",
    "      # forward pass\n",
    "      seq_logits, token_logits = model(input_ids, attention_mask)\n",
    "\n",
    "      all_seq_preds = np.append(all_seq_preds, seq_logits.argmax(-1).cpu())\n",
    "      all_seq_labels = np.append(all_seq_labels, seq_labels.cpu())\n",
    "\n",
    "      all_token_preds.append(token_logits.argmax(-1).cpu())\n",
    "      all_token_labels.append(F.pad(token_labels.cpu(), (0, token_logits.size(1) - token_labels.size(1)), \"constant\", -100))\n",
    "\n",
    "      # loss computation\n",
    "      seq_loss = F.cross_entropy(seq_logits, seq_labels)\n",
    "      token_loss = F.cross_entropy(token_logits.view(-1, token_logits.size(-1)),\n",
    "                                    F.pad(token_labels, (0, token_logits.size(1) - token_labels.size(1)), \"constant\", -100).view(-1), ignore_index=-100)\n",
    "\n",
    "      # update metrics\n",
    "      total_seq_loss_test += seq_loss.item()\n",
    "      total_token_loss_test += token_loss.item()\n",
    "\n",
    "  # compute and print test performance metrics  \n",
    "  print(f\"\\nTest \\t Sequence Loss: {total_seq_loss_test/len(test_dataloader)}, Token Loss: {total_token_loss_test/len(test_dataloader)}\")\n",
    "  f1_seq = f1_score(all_seq_labels, all_seq_preds, average=\"weighted\")\n",
    "  acc_seq = accuracy_score(all_seq_labels, all_seq_preds)\n",
    "  print(f\"\\t Sequence F_1: {f1_seq}, Accuracy: {acc_seq}\")\n",
    "  all_token_preds = np.vstack(all_token_preds)\n",
    "  all_token_labels = np.vstack(all_token_labels)\n",
    "  metrics_token = compute_metrics(all_token_preds, all_token_labels)\n",
    "  print(f\"\\t Token F_1: {metrics_token['f1']}, Accuracy: {metrics_token['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoDEkEK5gEmf"
   },
   "outputs": [],
   "source": [
    "lr = 5e-5 epochs = 14 alpha = 1 beta = 7.5\n",
    "\n",
    "Test \t Sequence Loss: 0.525999091565609, Token Loss: 0.16994309797883034\n",
    "\t Sequence F_1: 0.869773786585689, Accuracy: 0.87\n",
    "\t Token F_1: 0.849772382397572, Accuracy: 0.9657412876550502"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
